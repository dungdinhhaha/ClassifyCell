{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6ead02b",
      "metadata": {
        "id": "e6ead02b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "# Change to MyDrive first\n",
        "os.chdir('/content/drive/MyDrive')\n",
        "print('Current working directory for cloning:', os.getcwd())\n",
        "\n",
        "# Clone the repository directly into MyDrive\n",
        "!git clone https://github.com/dungdinhhaha/ClassifyCell /content/drive/MyDrive/PhatHienTeBao\n",
        "\n",
        "# Now change into the cloned repository for subsequent operations\n",
        "os.chdir('/content/drive/MyDrive/PhatHienTeBao')\n",
        "print('CWD:', os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcc2e970",
      "metadata": {
        "id": "dcc2e970"
      },
      "source": [
        "## 2) Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c8df0b9",
      "metadata": {
        "id": "1c8df0b9"
      },
      "outputs": [],
      "source": [
        "!pip install -q opencv-python pillow scikit-learn matplotlib seaborn\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "print(f'‚úÖ TensorFlow version: {tf.__version__}')\n",
        "print(f'‚úÖ Keras version: {keras.__version__}')\n",
        "print(f'‚úÖ NumPy version: {np.__version__}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b4585f0",
      "metadata": {
        "id": "3b4585f0"
      },
      "source": [
        "## 3) Load Images from Folders (Each Folder = 1 Class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3a5ff5c",
      "metadata": {
        "id": "d3a5ff5c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Define label map\n",
        "label_map = {\n",
        "    0: \"back_ground\",\n",
        "    1: 'ascus',\n",
        "    2: 'asch',\n",
        "    3: 'lsil',\n",
        "    4: 'hsil',\n",
        "    5: 'scc',\n",
        "    6: 'agc',\n",
        "    7: 'trichomonas',\n",
        "    8: 'candida',\n",
        "    9: 'flora',\n",
        "    10: 'herps',\n",
        "    11: 'actinomyces',\n",
        "}\n",
        "\n",
        "num_classes = len(label_map)\n",
        "print(f'Number of classes: {num_classes}')\n",
        "print('Label map:', label_map)\n",
        "\n",
        "# Image folder path - thay ƒë·ªïi theo v·ªã tr√≠ folder c·ªßa b·∫°n\n",
        "images_dir = '/content/drive/MyDrive/PhatHienTeBao/images'\n",
        "print(f'\\nLoading images from: {images_dir}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ca382f1",
      "metadata": {
        "id": "9ca382f1"
      },
      "outputs": [],
      "source": [
        "def load_images_from_folders(images_dir, label_map, target_size=128):\n",
        "    \"\"\"\n",
        "    Load images from folder structure:\n",
        "    images_dir/\n",
        "        1/  (ascus)\n",
        "            image1.jpg\n",
        "            image2.jpg\n",
        "        2/  (asch)\n",
        "            image3.jpg\n",
        "        ...\n",
        "    \"\"\"\n",
        "    all_images = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(f'üîÑ Loading images from {images_dir}...')\n",
        "\n",
        "    # Iterate through each class folder\n",
        "    for class_id in sorted(label_map.keys()):\n",
        "        class_name = label_map[class_id]\n",
        "        class_dir = os.path.join(images_dir, str(class_id))\n",
        "\n",
        "        if not os.path.exists(class_dir):\n",
        "            print(f'‚ö†Ô∏è  Folder not found: {class_dir}')\n",
        "            continue\n",
        "\n",
        "        # Get all image files in this class folder\n",
        "        image_files = []\n",
        "        for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.PNG']:\n",
        "            image_files.extend(Path(class_dir).glob(ext))\n",
        "\n",
        "        print(f'\\nLoading Class {class_id} ({class_name}): {len(image_files)} images')\n",
        "\n",
        "        # Load each image\n",
        "        for img_path in image_files:\n",
        "            try:\n",
        "                # Load image\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "                # Resize\n",
        "                img = img.resize((target_size, target_size))\n",
        "\n",
        "                # Convert to numpy - NO NORMALIZATION (model will do it)\n",
        "                img_array = np.array(img, dtype=np.float32)\n",
        "\n",
        "                all_images.append(img_array)\n",
        "                all_labels.append(class_id)\n",
        "            except Exception as e:\n",
        "                print(f'  ‚ùå Error loading {img_path}: {e}')\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X = np.array(all_images)\n",
        "    y = np.array(all_labels)\n",
        "\n",
        "    print(f'\\n‚úÖ Total images loaded: {len(all_images)}')\n",
        "    print(f'üìä Dataset shape: X={X.shape}, y={y.shape}')\n",
        "    print(f'üìä Pixel range: [{X.min():.1f}, {X.max():.1f}]')\n",
        "\n",
        "    # Print class distribution\n",
        "    print(f'\\nüìä Class distribution:')\n",
        "    unique, counts = np.unique(y, return_counts=True)\n",
        "    for cls, cnt in zip(unique, counts):\n",
        "        print(f'   Class {cls:2d} ({label_map.get(cls, \"unknown\"):15s}): {cnt:4d} samples')\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Load all images (224x224 for better transfer learning)\n",
        "X, y = load_images_from_folders(images_dir, label_map, target_size=224)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ece0fff9",
      "metadata": {
        "id": "ece0fff9"
      },
      "source": [
        "## 4) Visualize Sample Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d75347c",
      "metadata": {
        "id": "7d75347c"
      },
      "outputs": [],
      "source": [
        "# Show samples from each class\n",
        "fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, cls in enumerate(range(num_classes)):\n",
        "    if i >= 12:\n",
        "        break\n",
        "\n",
        "    # Find first sample of this class\n",
        "    idx = np.where(y == cls)[0]\n",
        "\n",
        "    if len(idx) > 0:\n",
        "        sample_idx = idx[0]\n",
        "        axes[i].imshow(X[sample_idx])\n",
        "        axes[i].set_title(f'Class {cls}: {label_map.get(cls, \"unknown\")}\\n({len(idx)} samples)')\n",
        "    else:\n",
        "        axes[i].text(0.5, 0.5, f'Class {cls}\\nNo samples',\n",
        "                    ha='center', va='center', transform=axes[i].transAxes)\n",
        "\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b268437",
      "metadata": {
        "id": "9b268437"
      },
      "source": [
        "## 5) Split Train/Val/Test & Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2240b41f",
      "metadata": {
        "id": "2240b41f"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# First split: 80% train+val, 20% test\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Second split: 80/20 of remaining = 64% train, 16% val\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f'Train set: {X_train.shape[0]} samples')\n",
        "print(f'Val set:   {X_val.shape[0]} samples')\n",
        "print(f'Test set:  {X_test.shape[0]} samples')\n",
        "\n",
        "print('\\n‚úÖ Data split complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cebbfdd7",
      "metadata": {
        "id": "cebbfdd7"
      },
      "source": [
        "## 6) Build Classification Model (Transfer Learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c0bf22e",
      "metadata": {
        "id": "6c0bf22e"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "import tensorflow as tf\n",
        "\n",
        "# Very light augmentation for cytology - preserve cell morphology\n",
        "train_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.05),  # Very small rotation - preserve cell orientation\n",
        "], name='augmentation')\n",
        "\n",
        "def build_classifier(input_shape=(224, 224, 3), num_classes=12):\n",
        "    # Use DenseNet121 - excellent for medical imaging with fine details\n",
        "    base_model = tf.keras.applications.DenseNet121(\n",
        "        input_shape=input_shape,\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "\n",
        "    # Freeze backbone completely first\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Input\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Preprocessing - simple normalization [0,1]\n",
        "    x = layers.Rescaling(1./255.0)(inputs)\n",
        "\n",
        "    # Backbone\n",
        "    x = base_model(x, training=False)\n",
        "\n",
        "    # Dense classifier head for cytology\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    x = layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    return model, base_model\n",
        "\n",
        "# Build model\n",
        "model, base_model = build_classifier(num_classes=num_classes)\n",
        "\n",
        "# Compile - use standard loss without label smoothing\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print('üìä Model: DenseNet121 (frozen) + Dense Head for Cytology')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4197557a",
      "metadata": {
        "id": "4197557a"
      },
      "source": [
        "## 7) Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5fe946a",
      "metadata": {
        "id": "c5fe946a"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import tensorflow as tf\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "print('üìä Class weights:')\n",
        "for cls, weight in class_weights_dict.items():\n",
        "    print(f'   Class {cls} ({label_map.get(cls, \"unknown\")}): {weight:.3f}')\n",
        "\n",
        "# Apply LIGHT augmentation to training data only\n",
        "print('\\nüîÑ Applying light augmentation to training data...')\n",
        "X_train_aug = train_augmentation(X_train, training=True).numpy()\n",
        "print(f'‚úÖ Training data augmented: {X_train_aug.shape}')\n",
        "\n",
        "# Create model directory\n",
        "model_save_dir = '/content/drive/MyDrive/PhatHienTeBao/models'\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        os.path.join(model_save_dir, 'best_cytology_model.keras'),\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=20,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_accuracy',\n",
        "        factor=0.3,\n",
        "        patience=8,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# PHASE 1: Train with frozen backbone - MORE EPOCHS for cytology\n",
        "print('\\nüöÄ PHASE 1: Training classifier head (backbone frozen)...\\n')\n",
        "print('   Note: Cervical cytology is challenging - may take 40-50 epochs\\n')\n",
        "\n",
        "history1 = model.fit(\n",
        "    X_train_aug, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=60,  # Increased from 50 to 60\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "best_val_acc = max(history1.history['val_accuracy'])\n",
        "print(f'\\n‚úÖ Phase 1 done! Best val accuracy: {best_val_acc:.2%}')\n",
        "\n",
        "# PHASE 2: Unfreeze and fine-tune if Phase 1 accuracy > 30%\n",
        "if best_val_acc > 0.30:\n",
        "    print(f'\\nüöÄ PHASE 2: Fine-tuning backbone (last 80 layers)...\\n')\n",
        "    base_model.trainable = True\n",
        "\n",
        "    # Unfreeze last 80 layers (more aggressive fine-tuning)\n",
        "    for layer in base_model.layers[:-80]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Recompile with very low learning rate for medical imaging\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-6),  # Even lower for better fine-tuning\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    history2 = model.fit(\n",
        "        X_train_aug, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=40,  # More epochs for fine-tuning\n",
        "        batch_size=16,\n",
        "        callbacks=callbacks,\n",
        "        class_weight=class_weights_dict,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(f'\\n‚úÖ Phase 2 done! Best val accuracy: {max(history2.history[\"val_accuracy\"]):.2%}')\n",
        "\n",
        "    # Combine histories\n",
        "    history = history1\n",
        "    for key in ['loss', 'accuracy', 'val_loss', 'val_accuracy']:\n",
        "        history.history[key].extend(history2.history[key])\n",
        "else:\n",
        "    print(f'\\n‚ö†Ô∏è  Phase 1 accuracy: {best_val_acc:.2%}')\n",
        "    print('   This is expected for cervical cytology - classes are very similar')\n",
        "    print('   Model is learning but needs more epochs or better features')\n",
        "    history = history1\n",
        "\n",
        "print('\\n‚úÖ Training complete!')\n",
        "print(f'üìä Final best validation accuracy: {max(history.history[\"val_accuracy\"]):.2%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26dc4f50",
      "metadata": {
        "id": "26dc4f50"
      },
      "source": [
        "## 8) Plot Training History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "125a6a1d",
      "metadata": {
        "id": "125a6a1d"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Loss\n",
        "ax1.plot(history.history['loss'], label='Train Loss', marker='o')\n",
        "ax1.plot(history.history['val_loss'], label='Val Loss', marker='s')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training & Validation Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy\n",
        "ax2.plot(history.history['accuracy'], label='Train Accuracy', marker='o')\n",
        "ax2.plot(history.history['val_accuracy'], label='Val Accuracy', marker='s')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Training & Validation Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f'\\nüìä Final Results:')\n",
        "print(f'   Train Accuracy: {history.history[\"accuracy\"][-1]:.4f}')\n",
        "print(f'   Val Accuracy:   {history.history[\"val_accuracy\"][-1]:.4f}')\n",
        "print(f'   Best Val Acc:   {max(history.history[\"val_accuracy\"]):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31342f06",
      "metadata": {
        "id": "31342f06"
      },
      "source": [
        "## 9) Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "661ff8c7",
      "metadata": {
        "id": "661ff8c7"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Get unique classes in test set\n",
        "unique_classes = np.unique(y_test)\n",
        "target_names = [label_map.get(i, f'Class {i}') for i in unique_classes]\n",
        "\n",
        "# Classification report\n",
        "print('Classification Report on TEST SET:')\n",
        "print(classification_report(y_test, y_pred_classes,\n",
        "                          labels=unique_classes,\n",
        "                          target_names=target_names))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_classes, labels=unique_classes)\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "           xticklabels=[label_map.get(i, f'{i}') for i in unique_classes],\n",
        "           yticklabels=[label_map.get(i, f'{i}') for i in unique_classes])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (Test Set)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "966b1609",
      "metadata": {
        "id": "966b1609"
      },
      "source": [
        "## 10) Visualize Predictions on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87258b9d",
      "metadata": {
        "id": "87258b9d"
      },
      "outputs": [],
      "source": [
        "# Show some test predictions\n",
        "fig, axes = plt.subplots(4, 4, figsize=(15, 15))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Get random samples from test set\n",
        "random_indices = np.random.choice(len(X_test), 16, replace=False)\n",
        "\n",
        "for idx, sample_idx in enumerate(random_indices):\n",
        "    img = X_test[sample_idx]\n",
        "    true_class = y_test[sample_idx]\n",
        "\n",
        "    # Normalize image for display [0, 1]\n",
        "    img_display = np.clip(img / 255.0, 0, 1)\n",
        "\n",
        "    # Predict\n",
        "    pred = model.predict(img[None], verbose=0)[0]\n",
        "    pred_class = np.argmax(pred)\n",
        "    pred_conf = pred[pred_class]\n",
        "\n",
        "    # Plot\n",
        "    axes[idx].imshow(img_display)\n",
        "\n",
        "    true_name = label_map.get(true_class, f'C{true_class}')\n",
        "    pred_name = label_map.get(pred_class, f'C{pred_class}')\n",
        "\n",
        "    status = '‚úì' if pred_class == true_class else '‚úó'\n",
        "    color = 'green' if pred_class == true_class else 'red'\n",
        "\n",
        "    title = f'{status} True: {true_name}\\nPred: {pred_name} ({pred_conf:.2f})'\n",
        "    axes[idx].set_title(title, color=color, fontweight='bold')\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31498260",
      "metadata": {
        "id": "31498260"
      },
      "source": [
        "## 11) Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "004c2fb6",
      "metadata": {
        "id": "004c2fb6"
      },
      "outputs": [],
      "source": [
        "# Save final model\n",
        "model_save_dir = '/content/drive/MyDrive/PhatHienTeBao/models'\n",
        "final_model_path = os.path.join(model_save_dir, 'cell_classifier_final.keras')\n",
        "model.save(final_model_path)\n",
        "print(f'‚úÖ Model saved to: {final_model_path}')\n",
        "\n",
        "# List saved models\n",
        "import os\n",
        "if os.path.exists(model_save_dir):\n",
        "    print(f'\\nüìÇ Models in {model_save_dir}:')\n",
        "    for f in os.listdir(model_save_dir):\n",
        "        if f.endswith('.keras'):\n",
        "            full_path = os.path.join(model_save_dir, f)\n",
        "            size_mb = os.path.getsize(full_path) / (1024*1024)\n",
        "            print(f'   ‚úì {f} ({size_mb:.2f} MB)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c90f05c",
      "metadata": {
        "id": "3c90f05c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.colors import ListedColormap\n",
        "import numpy as np\n",
        "\n",
        "if 'predictions' in locals() and len(predictions) > 0:\n",
        "    # Create heat map showing abnormal cells\n",
        "    print('üìä Creating visualization...')\n",
        "\n",
        "    # Create confidence map for abnormal classes (non-background, non-flora, non-candida)\n",
        "    # These are typically: ascus, asch, lsil, hsil, scc, agc, trichomonas, herps, actinomyces\n",
        "    normal_classes = [0, 9, 8]  # background, flora, candida\n",
        "    abnormal_classes = [1, 2, 3, 4, 5, 6, 7, 10, 11]  # All others\n",
        "\n",
        "    # Create grid for heat map\n",
        "    patch_size = 224\n",
        "    stride = 112\n",
        "\n",
        "    grid_width = width // stride\n",
        "    grid_height = height // stride\n",
        "\n",
        "    heat_map = np.zeros((grid_height, grid_width))\n",
        "    confidence_map = np.zeros((grid_height, grid_width))\n",
        "    class_map = np.zeros((grid_height, grid_width), dtype=int)\n",
        "\n",
        "    for pred in predictions:\n",
        "        grid_x = pred['x'] // stride\n",
        "        grid_y = pred['y'] // stride\n",
        "\n",
        "        # Mark abnormal cells with high confidence\n",
        "        is_abnormal = pred['class'] in abnormal_classes\n",
        "        heat_map[grid_y, grid_x] = 1.0 if is_abnormal else 0.0\n",
        "        confidence_map[grid_y, grid_x] = pred['confidence']\n",
        "        class_map[grid_y, grid_x] = pred['class']\n",
        "\n",
        "    # Plot\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # Heat map of abnormal cells\n",
        "    axes[0].imshow(heat_map, cmap='RdYlGn_r', vmin=0, vmax=1)\n",
        "    axes[0].set_title('Abnormal Cells Detection\\n(Red = Abnormal, Green = Normal)')\n",
        "    axes[0].set_xlabel('X')\n",
        "    axes[0].set_ylabel('Y')\n",
        "    plt.colorbar(axes[0].imshow(heat_map, cmap='RdYlGn_r', vmin=0, vmax=1), ax=axes[0])\n",
        "\n",
        "    # Confidence map\n",
        "    im = axes[1].imshow(confidence_map, cmap='hot')\n",
        "    axes[1].set_title('Prediction Confidence')\n",
        "    axes[1].set_xlabel('X')\n",
        "    axes[1].set_ylabel('Y')\n",
        "    plt.colorbar(im, ax=axes[1])\n",
        "\n",
        "    # Class map\n",
        "    im = axes[2].imshow(class_map, cmap='tab20', vmin=0, vmax=num_classes)\n",
        "    axes[2].set_title('Cell Type Distribution')\n",
        "    axes[2].set_xlabel('X')\n",
        "    axes[2].set_ylabel('Y')\n",
        "    plt.colorbar(im, ax=axes[2], label='Class')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print statistics\n",
        "    print(f'\\nüìà Statistics:')\n",
        "    print(f'   Total patches analyzed: {len(predictions)}')\n",
        "    print(f'   Abnormal patches: {int(heat_map.sum())} ({100*heat_map.sum()/heat_map.size:.1f}%)')\n",
        "    print(f'   Mean confidence: {confidence_map.mean():.3f}')\n",
        "    print(f'   High confidence (>0.8): {(confidence_map > 0.8).sum()} patches')\n",
        "\n",
        "    # ========== EXTRACT ABNORMAL CELLS ==========\n",
        "    print(f'\\nüî¥ ABNORMAL CELLS DETECTED:')\n",
        "    print(f'   {\"=\"*80}')\n",
        "\n",
        "    abnormal_predictions = [p for p in predictions if p['class'] in abnormal_classes]\n",
        "    print(f'   Total abnormal cells: {len(abnormal_predictions)}')\n",
        "    print(f'   {\"=\"*80}\\n')\n",
        "\n",
        "    # Sort by confidence (highest first)\n",
        "    abnormal_predictions_sorted = sorted(abnormal_predictions,\n",
        "                                        key=lambda x: x['confidence'],\n",
        "                                        reverse=True)\n",
        "\n",
        "    # Print coordinates and class info\n",
        "    for idx, pred in enumerate(abnormal_predictions_sorted[:20], 1):  # Show top 20\n",
        "        class_name = label_map.get(pred['class'], f\"Class {pred['class']}\")\n",
        "        print(f\"   {idx:2d}. Coords: ({pred['x']:5d}, {pred['y']:5d}) | \"\n",
        "              f\"Class: {class_name:15s} | Confidence: {pred['confidence']:.3f}\")\n",
        "\n",
        "    if len(abnormal_predictions) > 20:\n",
        "        print(f\"   ... and {len(abnormal_predictions) - 20} more\")\n",
        "\n",
        "else:\n",
        "    print('‚ö†Ô∏è  No predictions available. Run cell 12 first.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe4c954a",
      "metadata": {
        "id": "fe4c954a"
      },
      "source": [
        "## 14) Display Abnormal Cell Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d27fa82",
      "metadata": {
        "id": "2d27fa82"
      },
      "outputs": [],
      "source": [
        "# Display abnormal cell patches with their coordinates and predictions\n",
        "if 'predictions' in locals() and len(predictions) > 0:\n",
        "    normal_classes = [0, 9, 8]\n",
        "    abnormal_classes = [1, 2, 3, 4, 5, 6, 7, 10, 11]\n",
        "\n",
        "    # Get abnormal predictions sorted by confidence\n",
        "    abnormal_predictions = [p for p in predictions if p['class'] in abnormal_classes]\n",
        "    abnormal_predictions_sorted = sorted(abnormal_predictions,\n",
        "                                        key=lambda x: x['confidence'],\n",
        "                                        reverse=True)\n",
        "\n",
        "    num_to_show = min(12, len(abnormal_predictions_sorted))  # Show max 12\n",
        "\n",
        "    print(f'üì∏ Displaying top {num_to_show} abnormal cells (highest confidence)...\\n')\n",
        "\n",
        "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Re-open slide to extract patches\n",
        "    try:\n",
        "        slide = OpenSlide(svs_file)\n",
        "\n",
        "        # Use detail_level if available (from smart zoom), otherwise level 0\n",
        "        if 'detail_lvl' in locals() and detail_lvl is not None:\n",
        "            use_level = detail_lvl\n",
        "        elif 'level' in locals() and level is not None:\n",
        "            use_level = level\n",
        "        else:\n",
        "            use_level = 0\n",
        "\n",
        "        for idx, pred in enumerate(abnormal_predictions_sorted[:num_to_show]):\n",
        "            x, y = pred['x'], pred['y']\n",
        "            patch_size = 224\n",
        "\n",
        "            # Read patch from SVS file\n",
        "            region = slide.read_region((x, y), use_level, (patch_size, patch_size))\n",
        "            patch = np.array(region.convert('RGB'), dtype=np.float32)\n",
        "\n",
        "            # Normalize for display [0, 1]\n",
        "            patch_display = np.clip(patch / 255.0, 0, 1)\n",
        "\n",
        "            # Get prediction details\n",
        "            class_id = pred['class']\n",
        "            class_name = label_map.get(class_id, f'Class {class_id}')\n",
        "            confidence = pred['confidence']\n",
        "\n",
        "            # Plot\n",
        "            axes[idx].imshow(patch_display)\n",
        "            axes[idx].set_title(\n",
        "                f'üî¥ {class_name}\\nConfidence: {confidence:.3f}\\nLocation: ({x}, {y})',\n",
        "                fontsize=10, fontweight='bold', color='darkred'\n",
        "            )\n",
        "            axes[idx].axis('off')\n",
        "\n",
        "            # Add red border\n",
        "            for spine in axes[idx].spines.values():\n",
        "                spine.set_edgecolor('red')\n",
        "                spine.set_linewidth(3)\n",
        "\n",
        "        slide.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'‚ùå Error reading patches: {e}')\n",
        "        print('‚ö†Ô∏è  Trying fallback method...')\n",
        "\n",
        "        # Fallback: If OpenSlide fails, try PIL\n",
        "        try:\n",
        "            img = Image.open(svs_file)\n",
        "            img_array = np.array(img.convert('RGB'), dtype=np.float32)\n",
        "\n",
        "            for idx, pred in enumerate(abnormal_predictions_sorted[:num_to_show]):\n",
        "                x, y = pred['x'], pred['y']\n",
        "                patch_size = 224\n",
        "\n",
        "                # Extract patch from image array\n",
        "                patch = img_array[y:y+patch_size, x:x+patch_size]\n",
        "                patch_display = np.clip(patch / 255.0, 0, 1)\n",
        "\n",
        "                class_id = pred['class']\n",
        "                class_name = label_map.get(class_id, f'Class {class_id}')\n",
        "                confidence = pred['confidence']\n",
        "\n",
        "                axes[idx].imshow(patch_display)\n",
        "                axes[idx].set_title(\n",
        "                    f'üî¥ {class_name}\\nConfidence: {confidence:.3f}\\nLocation: ({x}, {y})',\n",
        "                    fontsize=10, fontweight='bold', color='darkred'\n",
        "                )\n",
        "                axes[idx].axis('off')\n",
        "\n",
        "                for spine in axes[idx].spines.values():\n",
        "                    spine.set_edgecolor('red')\n",
        "                    spine.set_linewidth(3)\n",
        "        except Exception as e2:\n",
        "            print(f'‚ùå Fallback also failed: {e2}')\n",
        "\n",
        "    # Hide remaining subplots\n",
        "    for idx in range(num_to_show, len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Save coordinates to file\n",
        "    print(f'\\nüíæ Saving abnormal cell coordinates...')\n",
        "    output_path = '/content/drive/MyDrive/PhatHienTeBao/abnormal_cells_coordinates.txt'\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        f.write(f\"{'='*80}\\n\")\n",
        "        f.write(f\"ABNORMAL CELLS DETECTED - COORDINATES AND PREDICTIONS\\n\")\n",
        "        f.write(f\"{'='*80}\\n\")\n",
        "        f.write(f\"File: {svs_file}\\n\")\n",
        "        f.write(f\"Total abnormal cells found: {len(abnormal_predictions)}\\n\")\n",
        "        f.write(f\"Slide dimensions: {width} x {height}\\n\")\n",
        "        f.write(f\"{'='*80}\\n\\n\")\n",
        "\n",
        "        for idx, pred in enumerate(abnormal_predictions_sorted, 1):\n",
        "            x, y = pred['x'], pred['y']\n",
        "            class_id = pred['class']\n",
        "            class_name = label_map.get(class_id, f'Class {class_id}')\n",
        "            confidence = pred['confidence']\n",
        "\n",
        "            f.write(f\"{idx:4d}. X={x:6d}, Y={y:6d} | Class: {class_name:15s} | Confidence: {confidence:.4f}\\n\")\n",
        "\n",
        "    print(f'‚úÖ Coordinates saved to: {output_path}')\n",
        "\n",
        "elif 'predictions' not in locals() or len(predictions) == 0:\n",
        "    print('‚ö†Ô∏è  No predictions available. Run cell 12 first.')\n",
        "else:\n",
        "    print('‚ö†Ô∏è  Could not load slide file.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e16f5472",
      "metadata": {
        "id": "e16f5472"
      },
      "source": [
        "## 13) Visualize Heat Map of Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd1ca47c",
      "metadata": {
        "id": "dd1ca47c"
      },
      "outputs": [],
      "source": [
        "## 12) Predict on Whole Slide Image (SVS)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}